# parse_its_book.py  (ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ ĞºĞ½Ğ¸Ğ³Ñƒ/ÑĞ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ğ¸Ğº Ñ its.1c.ru Ğ² Markdown-Ñ„Ğ°Ğ¹Ğ»Ñ‹.

ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚:
  â€¢ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ Â«ĞºĞ½Ğ¸Ğ¶Ğ½Ñ‹ĞµÂ» Ğ±Ğ°Ğ·Ñ‹   /db/<code>/content/â€¦          (unfdoc, pubâ€¦)
  â€¢ ÑĞ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ğ¸Ğº metod81       /db/metod81/browse/â€¦|content/â€¦

ĞŸĞ¾ÑĞ»Ğµ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° ÑĞºÑ€Ğ¸Ğ¿Ñ‚ ÑĞ¿Ñ€Ğ¾ÑĞ¸Ñ‚:
  âœ ĞºĞ¾Ğ´ Ğ±Ğ°Ğ·Ñ‹  (Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€: unfdoc | pubchaos2order | metod81)
  âœ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¿Ğ°Ğ¿ĞºÑƒ Ğ´Ğ»Ñ Markdown
  âœ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ / Ñ€Ğ°Ğ·Ğ´ĞµĞ»  â€“ ÑƒĞ¹Ğ´ÑƒÑ‚ Ğ² YAML.

Ğ”Ğ»Ñ *metod81* Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ¿Ğ°Ñ€ÑĞ¸Ñ‚ÑÑ **Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾** Ğ²ĞµÑ‚ĞºĞ° nav_2503
(Â«Ğ Ğ°Ğ±Ğ¾Ñ‡ĞµĞµ Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ°ÑÑĞ¸Ñ€Ğ°â€¦Â»). Ğ•ÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ° Ğ´Ñ€ÑƒĞ³Ğ°Ñ â€“ Ğ¿Ğ¾Ğ¼ĞµĞ½ÑĞ¹Ñ‚Ğµ ROOT_NAV_ID.
"""
from pathlib import Path
import re, time
import json
from collections import deque
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright, TimeoutError
DEBUG = True   # global flag, used for verbose logging

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹
BOOK      = input("ğŸ“˜ ĞšĞ¾Ğ´ Ğ±Ğ°Ğ·Ñ‹ (unfdoc / metod81 / â€¦): ").strip()
OUT_DIR   = Path(input("ğŸ“ ĞšÑƒĞ´Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ md-Ñ„Ğ°Ğ¹Ğ»Ñ‹: ").strip())
CATEGORY  = input("ğŸ· ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ (YAML): ").strip()
SECTION   = input("ğŸ“š Ğ Ğ°Ğ·Ğ´ĞµĞ» 1Ğ¡ (YAML): ").strip()

OUT_DIR.mkdir(parents=True, exist_ok=True)
START_URL      = f"https://its.1c.ru/db/{BOOK}"
ROOT_NAV_ID    = "2503"          # Ğ´Ğ»Ñ metod81: Â«Ğ Ğ°Ğ±Ğ¾Ñ‡ĞµĞµ Ğ¼ĞµÑÑ‚Ğ¾ ĞºĞ°ÑÑĞ¸Ñ€Ğ°â€¦Â»

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Ğ¼ĞµĞ»ĞºĞ¸Ğµ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹
def sanitize(t: str) -> str:
    t = re.sub(r"[^\w\- ]", "", t.lower()).replace(" ", "-")
    t = t.replace("/", "-").replace("\\", "-")
    return t[:100] or "untitled"

def normalize(u: str, keep_hash=False) -> str:
    if "#" in u and not keep_hash:  u = u.split("#")[0]
    return u.rstrip("/")

def save_md(title, url, text, subcat_slug, file_slug, date_str):
    """
    Write a markdown file with YAML frontâ€‘matter.

    Parameters
    ----------
    title : str            â€“ document human title
    url   : str            â€“ canonical ITS URL
    text  : str            â€“ main markdown body
    subcat_slug : str      â€“ slug to be written in YAML as â€œsubcategoryâ€
    file_slug    : str     â€“ slug used for the *.md filename
    date_str     : str     â€“ dd.mm.yyyy taken from the page
    """
    # JSONâ€‘style quoting guarantees proper escaping of inner quotes
    q_str  = json.dumps(title, ensure_ascii=False)
    url_str = json.dumps(url,   ensure_ascii=False)

    fp = OUT_DIR / f"{file_slug}.md"
    fp.parent.mkdir(parents=True, exist_ok=True)     # ensure path
    if fp.exists():          # duplicate â€“ skip writing
        return

    fp.write_text(
        f"""---
date: "{date_str}"
category: {CATEGORY}
section_1c: {SECTION}
subcategory: {subcat_slug}
question: {q_str}
url: {url_str}
---

{text.strip()}
""",
        "utf-8"
    )
    print("   âœ…", fp.name)

def log_snip(t, txt):
    print("      â†³", (txt.strip().replace("\n", " ") or "<EMPTY>")[:80])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Playwright helpers
def wait_doc_frame(page, timeout=15_000):
    """Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ iframe Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼, Ğ»Ğ¸Ğ±Ğ¾ None (Ğ´Ğ»Ñ metod81 paywall-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†)."""
    try:
        page.wait_for_selector('iframe[name="w_metadata_doc_frame"]',
                               state="attached", timeout=timeout)
        page.wait_for_function(
            """sel=>{const e=document.querySelector(sel);
               return e&&!e.hidden&&e.getAttribute('loading')!=='true'}""",
            arg='iframe[name="w_metadata_doc_frame"]', timeout=timeout)
        return page.frame(name="w_metadata_doc_frame")
    except TimeoutError:
        return None

def goto_and_get_node(page, url):
    page.goto(url, timeout=30_000, wait_until="domcontentloaded")
    time.sleep(.3)                                         # Ğ¼Ğ¸ĞºÑ€Ğ¾Ğ¿Ğ°ÑƒĞ·Ğ° Ğ´Ğ»Ñ JS
    frame = wait_doc_frame(page) or page                   # fallback Ğº ÑĞ°Ğ¼Ğ¾Ğ¼Ñƒ page
    try:
        frame.wait_for_selector("h1,h2,h3,p", timeout=8_000)
    except TimeoutError:
        pass
    return frame

def node_html(node) -> str:      # page Ğ¸ frame Ğ¸Ğ¼ĞµÑÑ‚ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¹ .content()
    return node.content()

def extract_plain(node) -> str:
    soup = BeautifulSoup(node_html(node), "html.parser")
    div  = soup.select_one("div.doc-content,#content,body") or soup
    txt  = div.get_text("\n", strip=True)
    # ÑƒĞ±Ğ¸Ñ€Ğ°ĞµĞ¼ ÑĞ»ÑƒĞ¶ĞµĞ±Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ¾ĞºÑƒâ€‘Â«ÑˆĞ°Ğ¿ĞºÑƒÂ» Ñ‚Ğ¸Ğ¿Ğ°
    # â€œĞĞ±Ñ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Â Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½ â€¦ Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Â Ğ´Ğ¾ 14.10.2025 â€¦â€
    txt  = re.sub(r"^ĞĞ±Ñ‰Ğ¸Ğ¹ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ.*?Ğ”Ğ¾ÑÑ‚ÑƒĞ¿ Ğ´Ğ¾ \d{2}\.\d{2}\.\d{4}\s+", "",
                  txt, flags=re.S)
    return txt

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° h2/h3-Ğ¿Ğ¾Ğ´Ğ±Ğ»Ğ¾ĞºĞ¸
def split_sections(html: str):
    soup = BeautifulSoup(html, "html.parser")
    cur, buf, out = None, [], []
    is_hdr = lambda n: n.name in ("h2","h3")
    for n in soup.find_all(["h2","h3","p"]):
        if is_hdr(n):
            if cur and buf:
                out.append((cur, "\n".join(buf).strip()))
                buf=[]
            cur = n.get_text(" ", strip=True)
        else:
            txt=n.get_text(" ", strip=True)
            if txt: buf.append(txt)
    if cur and buf: out.append((cur, "\n".join(buf).strip()))
    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ
with sync_playwright() as p:
    br = p.chromium.launch(headless=False, slow_mo=80)
    ctx, pg = br.new_context(), br.new_page()

    pg.goto("https://its.1c.ru/")
    input("â³ ĞŸÑ€Ğ¾Ğ¹Ğ´Ğ¸Ñ‚Ğµ DDoS/Ğ»Ğ¾Ğ³Ğ¸Ğ½ Ğ¸ Enterâ€¦ ")

    # ĞµÑĞ»Ğ¸ metod81 â€“ ÑÑ€Ğ°Ğ·Ñƒ Ğ² Ğ½ÑƒĞ¶Ğ½ÑƒÑ Ğ²ĞµÑ‚ĞºÑƒ
    if BOOK=="metod81":
        START_URL = f"https://its.1c.ru/db/metod81/browse/13/-1/2115/{ROOT_NAV_ID}"
    pg.goto(START_URL, wait_until="domcontentloaded")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ÑÑÑ‹Ğ»ĞºĞ¸ TOC
    if BOOK=="metod81":
        print("ğŸ” Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°Ñ ÑÑÑ‹Ğ»ĞºĞ¸ Ğ² Ğ²ĞµÑ‚ĞºĞµ nav_"+ROOT_NAV_ID)
        toc_links = pg.evaluate(f"""
        (()=>{{
          const root=document.querySelector('#nav_{ROOT_NAV_ID}');
          if(!root) return [];
          return Array.from(root.querySelectorAll('a[href]')).map(a=>{{
            const href=a.getAttribute('href');
            return {{
              title:a.textContent.trim(),
              url  :href.startsWith('http')?href:new URL(href,'https://its.1c.ru').href
            }};
          }});
        }})()
        """)
        # â”€â”€ Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¢ĞĞ›Ğ¬ĞšĞ ÑÑÑ‹Ğ»ĞºĞ¸, Ğ¾Ñ‚Ğ½Ğ¾ÑÑÑ‰Ğ¸ĞµÑÑ Ğº Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ²ĞµÑ‚ĞºĞµ
        toc_links = [l for l in toc_links
                     if f"/{ROOT_NAV_ID}" in l["url"]          # ÑĞ°Ğ¼Ğ¸ browseâ€‘ÑƒĞ·Ğ»Ñ‹ Ğ²ĞµÑ‚ĞºĞ¸
                     or "/content/" in l["url"]]               # Ğ»Ğ¸Ğ±Ğ¾ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğµ hdocâ€‘ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
    else:   # Ğ¿Ñ€ĞµĞ¶Ğ½ÑÑ ĞºĞ½Ğ¸Ğ¶Ğ½Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ°
        pg.wait_for_selector(f'#w_metadata_toc a[href*="/db/{BOOK}/content/"]',
                             timeout=15_000)
        toc_links = pg.evaluate(f"""
        Array.from(document.querySelectorAll(
               '#w_metadata_toc a[href*="/db/{BOOK}/content/"]'))
             .map(a=>({{title:a.textContent.trim(),
                       url:new URL(a.getAttribute('href'),
                                   'https://its.1c.ru').href}}));
        """)

    links = [l for l in toc_links if l["title"] and l["url"]]
    print(f"ğŸ“‹ ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ ÑÑÑ‹Ğ»Ğ¾Ğº: {len(links)}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ base-Ğ¸Ğ¼ĞµĞ½Ğ°
    chap, sub_idx = None, 0
    for ln in links:
        t=ln["title"]
        m=re.match(r"Ğ“Ğ»Ğ°Ğ²Ğ°\s+(\d+)\.", t, flags=re.I)
        if m:
            chap=int(m.group(1)); sub_idx=0
            ln["fname_base"]=f"Ğ³Ğ»Ğ°Ğ²Ğ°-{chap}-{sanitize(t[m.end():])}"
        elif chap:
            sub_idx+=1
            ln["fname_base"]=f"Ğ³Ğ»Ğ°Ğ²Ğ°-{chap}-{sub_idx}-{sanitize(t)}"
        else:
            ln["fname_base"]=sanitize(t)

    queue = deque(links)          # ÑÑÑ‹Ğ»ĞºĞ¸ Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ
    done_urls: set[str] = set()   # ÑƒĞ¶Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğµ URL

    while queue:
        ln = queue.popleft()
        url=normalize(ln["url"])

        # ---------- metod81: browseâ†’ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² ----------
        # Ğ”Ğ»Ñ ÑĞ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° metod81 ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ /browse/â€¦ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ÑĞ¿Ğ¸ÑĞ¾Ğº hdocâ€‘ÑÑÑ‹Ğ»Ğ¾Ğº.
        # Ğ˜Ñ… ÑĞ°Ğ¼Ğ¸Ñ… Ğ¼Ñ‹ Ğ½Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ â€“ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ ÑÑ‚Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ğµ /content/â€¦ Ğ² Ğ¾Ñ‡ĞµÑ€ĞµĞ´ÑŒ.
        if BOOK == "metod81" and "/content/" not in url:
            node = goto_and_get_node(pg, url)        # Ğ¾Ñ‚Ñ€Ğ¸ÑĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ browseâ€‘ÑƒĞ·ĞµĞ»
            docs = node.evaluate("""
                Array.from(
                    document.querySelectorAll(
                        '#w_metadata_navlist a[href*="/content/"]'
                    )
                ).map(a => {
                    const href = a.getAttribute('href') || '';
                    return {
                        title: (a.textContent || '').trim(),
                        url  : href.startsWith('http') ? href
                               : new URL(href, 'https://its.1c.ru').href
                    };
                });
            """)
            added = 0
            for d in docs:
                nurl = normalize(d["url"])
                if nurl in done_urls:
                    continue
                d["fname_base"] = sanitize(d["title"])
                d["subcategory"] = sanitize(ln["title"])
                queue.append(d)
                added += 1
            if DEBUG:
                print(f"   â• Ğ¸Ğ· ÑĞ¿Ğ¸ÑĞºĞ° Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²: {added}")
            done_urls.add(url)     # ÑĞ°Ğ¼ browseâ€‘url Ğ¿Ğ¾Ğ¼ĞµÑ‡Ğ°ĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¼
            continue               # Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ğ¼ Ğº ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼Ñƒ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñƒ Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ¸

        if url in done_urls: continue
        node = goto_and_get_node(pg, url)
        html = node_html(node)
        soup = BeautifulSoup(html, "html.parser")
        date_elem = soup.select_one("span.date")
        date_str = date_elem.get_text(strip=True) if date_elem else ""
        text = extract_plain(node)
        log_snip(ln["title"], text)
        if text:
            # prefer an explicit subâ€‘category from the queue element,
            # otherwise fall back to the file slug
            subcat = ln.get("subcategory", ln["fname_base"])
            save_md(ln["title"], url, text, subcat, ln["fname_base"], date_str)
        # Ğ¿Ğ¾Ğ´-Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸
        done_urls.add(url)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ñ€ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ñ‹Ğµ ÑÑÑ‹Ğ»ĞºĞ¸ Ñ‚Ğ¾Ğ¹ Ğ¶Ğµ Ğ±Ğ°Ğ·Ñ‹
        if BOOK != "metod81":
            try:
                # Ğ”Ğ»Ñ Â«ĞºĞ½Ğ¸Ğ¶Ğ½Ñ‹Ñ…Â» Ğ±Ğ°Ğ· Ğ±ĞµÑ€Ñ‘Ğ¼ Ğ»ÑĞ±Ñ‹Ğµ ÑÑÑ‹Ğ»ĞºĞ¸ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ñ‚Ğ¾Ğ¹â€‘Ğ¶Ğµ db/<BOOK>/ â€¦
                # Ğ”Ğ»Ñ metod81â€Šâ€”â€ŠĞ±ĞµÑ€Ñ‘Ğ¼ Ğ¢ĞĞ›Ğ¬ĞšĞ ĞºĞ¾Ğ½ĞµÑ‡Ğ½Ñ‹Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ /content/ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°.
                if BOOK == "metod81":
                    sel = 'a[href*="/db/metod81/content/"]'
                else:
                    sel = f'a[href*="/db/{BOOK}/"]'

                raw = node.evaluate(f"""
                    Array.from(document.querySelectorAll("{sel}")).map(a => {{
                        const href = a.getAttribute("href") || "";
                        return {{
                            title : (a.textContent || "").trim(),
                            url   : href.startsWith("http") ? href
                                    : new URL(href, "https://its.1c.ru").href
                        }};
                    }});
                """)

                added = 0
                for r in raw:
                    nurl = normalize(r["url"], keep_hash=True)
                    if nurl in done_urls:
                        continue

                    # Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ±Ğ°Ğ·Ñƒ Ğ¸Ğ¼ĞµĞ½Ğ¸: Ğ½Ğ°ÑĞ»ĞµĞ´ÑƒĞµĞ¼ Ğ¸Ğ¼Ñ Ñ€Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»Ñ + ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº
                    parent_base = ln.get("fname_base", sanitize(ln["title"]))
                    r["fname_base"] = f"{parent_base}-{sanitize(r['title'])}"
                    queue.append(r)
                    added += 1

                if added and DEBUG:
                    print(f"   â• Ğ´Ğ¾Ñ‡ĞµÑ€Ğ½Ğ¸Ñ… ÑÑÑ‹Ğ»Ğ¾Ğº: {added}")
            except Exception:
                pass

    print("ğŸ Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾")
    br.close()