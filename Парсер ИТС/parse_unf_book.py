# –°–∫—Ä–∏–ø—Ç: parse_unf_book.py
# –¶–µ–ª—å: —Å–ø–∞—Ä—Å–∏—Ç—å –∫–Ω–∏–≥—É —Å https://its.1c.ru/db/unfdoc –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown –¥–ª—è RAG

import re
from pathlib import Path
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright

BASE_URL = "https://its.1c.ru"
START_URL = f"{BASE_URL}/db/unfdoc"
USERNAME = "mors"
PASSWORD = "morser"
OUTPUT_DIR = Path("data/unfdoc/md")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


def extract_content(page, url):
    try:
        page.goto(url, timeout=20000)
        page.wait_for_selector("iframe[name=w_metadata_doc_frame]", timeout=20000)
        iframe = page.frame(name="w_metadata_doc_frame")
        if not iframe:
            print(f"‚ùå iframe –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {url}")
            return None

        html = iframe.content()
        soup = BeautifulSoup(html, "html.parser")
        content_div = soup.select_one("div.doc-content") or soup.select_one("div#content") or soup.body
        if not content_div:
            print(f"‚ö†Ô∏è –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {url}")
            debug_name = "debug_failed_" + sanitize_filename(url.split("/")[-1]) + ".html"
            with open(debug_name, "w", encoding="utf-8") as f:
                f.write(html)
            return None

        return content_div.get_text("\n", strip=True).strip()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e}")
        import traceback; traceback.print_exc()
        return None


def sanitize_filename(text):
    text = text.lower()
    text = re.sub(r"[^\w\- ]", "", text)
    text = text.replace(" ", "-")
    return text[:100]


def save_as_md(title, url, content):
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä—É "–≥–ª–∞–≤–∞-<–Ω–æ–º–µ—Ä>-<–ø–æ–¥–Ω–æ–º–µ—Ä>-–Ω–∞–∑–≤–∞–Ω–∏–µ"
    chapter_match = re.match(r"^(\d+)(?:\.(\d+))?\.\s*(.+)$", title)
    if chapter_match:
        main, sub, name = chapter_match.groups()
        if sub:
            filename = f"–≥–ª–∞–≤–∞-{main}-{sub}-{sanitize_filename(name)}.md"
        else:
            filename = f"–≥–ª–∞–≤–∞-{main}-{sanitize_filename(name)}.md"
    elif title.lower().startswith("–≥–ª–∞–≤–∞ "):
        filename = f"{sanitize_filename(title)}.md"
    else:
        filename = f"{sanitize_filename(title)}.md"

    out_path = OUTPUT_DIR / filename
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(f"""---
category: SD
section_1c: –£–ù–§
question: {title}
url: {url}
---

{content.strip()}
""")
    print("‚úÖ", filename)


def extract_content_from_iframe(iframe):
    html = iframe.content()
    soup = BeautifulSoup(html, "html.parser")
    div = soup.select_one("div.doc-content, div#content") or soup.body
    return div.get_text("\n", strip=True) if div else ""


def get_doc_iframe(page):
    page.wait_for_selector("iframe[name=w_metadata_doc_frame]", timeout=10000)
    return page.frame(name="w_metadata_doc_frame")


def safe_goto(page, url):
    """–ü–µ—Ä–µ—Ö–æ–¥–∏—Ç –ø–æ url –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω—ã–π iframe —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º."""
    page.goto(url, timeout=20000)
    return get_doc_iframe(page)


def main():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False, slow_mo=100)
        context = browser.new_context()
        page = context.new_page()

        # 1. –ó–∞—â–∏—Ç–∞ + –ª–æ–≥–∏–Ω
        page.goto("https://its.1c.ru/")
        input("‚è≥ –ü—Ä–æ–π–¥–∏ –∑–∞—â–∏—Ç—É DDoS –∏ –Ω–∞–∂–º–∏ Enter...")
        print("üß≠ –¢–µ–∫—É—â–∏–π URL:", page.url)
        with open("debug_page_content.html", "w", encoding="utf-8") as f:
            f.write(page.content())

        # 2. –ü–µ—Ä–µ—Ö–æ–¥ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –£–ù–§
        page.goto(START_URL)
        page.wait_for_timeout(3000)  # –¥–∞—Ç—å –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–æ–≥—Ä—É–∑–∫—É –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        print("üß≠ –ü–æ—Å–ª–µ –ª–æ–≥–∏–Ω–∞:", page.url)

        # DEBUG: —Å–æ—Ö—Ä–∞–Ω—è–µ–º HTML
        with open("debug_after_login.html", "w", encoding="utf-8") as f:
            f.write(page.content())

        print("üì¶ –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ—Ä–µ–π–º—ã:")
        for f in page.frames:
            print("  ‚Üí", f.name, f.url)
        try:
            page.wait_for_selector("iframe[name=w_metadata_doc_frame]", timeout=10000)
            iframe = page.frame(name="w_metadata_doc_frame")
            if not iframe:
                print("‚ùå iframe w_metadata_doc_frame –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return
            print(f"üß≠ iframe –∑–∞–≥—Ä—É–∂–µ–Ω: {iframe.url}")
            with open("debug_iframe_index.html", "w", encoding="utf-8") as f:
                f.write(iframe.content())
            soup = BeautifulSoup(iframe.content(), "html.parser")
            ul_count = len(soup.select("ul"))
            li_count = len(soup.select("li"))
            a_count = len(soup.select("a"))
            print(f"üìä –≠–ª–µ–º–µ–Ω—Ç—ã –≤ iframe: ul={ul_count}, li={li_count}, a={a_count}")

            print("üì• –ü—Ä–æ–≤–µ—Ä–∏–º HTML –¥–æ —Ä–∞—Å–∫—Ä—ã—Ç–∏—è –ø–∞–ø–æ–∫...")
            with open("debug_iframe_before_expand.html", "w", encoding="utf-8") as f:
                f.write(iframe.content())

            iframe.wait_for_selector("a[href*='content']", timeout=10000)
            print("üîç –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å 'content'...")
            toc_links = iframe.evaluate("""
                Array.from(document.querySelectorAll('a[href*="/db/unfdoc/content/"]'))
                     .map(a => ({title: a.textContent.trim(), url: a.href}))
            """)

            links = [link for link in toc_links if link["title"] and link["url"] and not link["url"].startswith("#")]

            with open("debug_iframe_links.html", "w", encoding="utf-8") as f:
                f.write(iframe.content())
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–∂–∏–¥–∞–Ω–∏—è —Å—Å—ã–ª–æ–∫ –≤ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–∏: {e}")
            with open("debug_unfdoc_page.html", "w", encoding="utf-8") as f:
                f.write(page.content())
            return

        print(f"üìã –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ –≤ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–∏: {len(links)}")

        print(f"üß© –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(links)}")

        saved_urls = set()

        for i, link in enumerate(links):
            print(f"üîπ [{i+1}/{len(links)}] {link['title']} ‚Äî {link['url']}")
            iframe = safe_goto(page, link["url"])
            content = extract_content_from_iframe(iframe)
            if content:
                save_as_md(link["title"], link["url"], content)
                saved_urls.add(link["url"])

            # --- –∏—â–µ–º –ø–æ–¥–≥–ª–∞–≤—ã (–≤—Å–µ —Å—Å—ã–ª–∫–∏ content/ –≤–Ω—É—Ç—Ä–∏ iframe) ---
            try:
                sub_links = iframe.evaluate("""
                    Array.from(document.querySelectorAll('a[href*="/db/unfdoc/content/"]'))
                         .map(a => ({ title: a.textContent.trim(), url: a.href }));
                """)
                print(f"üîñ –ü–æ–¥-—Å—Å—ã–ª–æ–∫ –Ω–∞–π–¥–µ–Ω–æ: {len(sub_links)}")

                for sub in sub_links:
                    if (not sub["title"] or not sub["url"]
                        or sub["url"] == link["url"]
                        or sub["url"] in saved_urls):
                        continue

                    print(f"üìÅ –ü–æ–¥–≥–ª–∞–≤–∞: {sub['title']} ‚Äî {sub['url']}")
                    iframe = safe_goto(page, sub["url"])
                    sub_content = extract_content_from_iframe(iframe)
                    if sub_content:
                        save_as_md(sub["title"], sub["url"], sub_content)
                        saved_urls.add(sub["url"])
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ–¥–≥–ª–∞–≤: {e}")

        print("üèÅ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω. –ó–∞–∫—Ä—ã–≤–∞–µ–º –±—Ä–∞—É–∑–µ—Ä.")
        browser.close()


if __name__ == "__main__":
    main()
