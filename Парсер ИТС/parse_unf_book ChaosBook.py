# –°–∫—Ä–∏–ø—Ç: parse_unf_book.py
# –¶–µ–ª—å: —Å–ø–∞—Ä—Å–∏—Ç—å –∫–Ω–∏–≥—É —Å https://its.1c.ru/db/unfdoc –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown –¥–ª—è RAG

from pathlib import Path
import re
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
from collections import deque

DEBUG = True


# üîΩ –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
book_code = input("üìò –í–≤–µ–¥–∏—Ç–µ –∫–æ–¥ –∫–Ω–∏–≥–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, unfdoc –∏–ª–∏ pubchaos2order): ").strip()
out_dir = input("üìÅ –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, data/ChaosBook/md): ").strip()
category = input("üè∑ –ö–∞—Ç–µ–≥–æ—Ä–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ö–Ω–∏–≥–∏): ").strip()
section = input("üìö –†–∞–∑–¥–µ–ª 1–° (–Ω–∞–ø—Ä–∏–º–µ—Ä, –£–ù–§ –∏–ª–∏ Chaos ‚Üí Order): ").strip()

# üîΩ –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–≤–µ–¥—ë–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
BOOK = book_code
START_URL = f"https://its.1c.ru/db/{BOOK}"
OUTPUT_DIR = Path(out_dir)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

BASE_URL = "https://its.1c.ru"
# START_URL = f"{BASE_URL}/db/unfdoc"
# OUTPUT_DIR = Path("data/SD/md")
# OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


def extract_content(page, url):
    try:
        page.goto(url, timeout=20000)
        page.wait_for_selector("iframe[name=w_metadata_doc_frame]", timeout=20000)
        iframe = page.frame(name="w_metadata_doc_frame")
        if not iframe:
            print(f"‚ùå iframe –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {url}")
            return None

        html = iframe.content()
        soup = BeautifulSoup(html, "html.parser")
        content_div = soup.select_one("div.doc-content") or soup.select_one("div#content") or soup.body
        if not content_div:
            print(f"‚ö†Ô∏è –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {url}")
            debug_name = "debug_failed_" + sanitize_filename(url.split("/")[-1]) + ".html"
            with open(debug_name, "w", encoding="utf-8") as f:
                f.write(html)
            return None

        return content_div.get_text("\n", strip=True).strip()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e}")
        import traceback; traceback.print_exc()
        return None


def sanitize_filename(text):
    text = text.lower()
    text = re.sub(r"[^\w\- ]", "", text)
    text = text.replace(" ", "-")
    return text[:100]


def normalize_url(u: str, *, keep_fragment: bool = False) -> str:
    """
    –ü—Ä–∏–≤–æ–¥–∏—Ç —Å—Å—ã–ª–∫—É –∫ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–º—É –≤–∏–¥—É.
    ‚Ä¢ –ø–æ‚Äë—É–º–æ–ª—á–∞–Ω–∏—é —É–±–∏—Ä–∞–µ—Ç —á–∞—Å—Ç—å –ø–æ—Å–ª–µ `#`, –Ω–æ –ø—Ä–∏ keep_fragment=True –æ—Å—Ç–∞–≤–ª—è–µ—Ç –µ—ë;
    ‚Ä¢ —Å—Ä–µ–∑–∞–µ—Ç –∑–∞–≤–µ—Ä—à–∞—é—â–∏–π —Å–ª—ç—à.
    """
    u = u.strip()
    if "#" in u and not keep_fragment:
        u = u.split("#")[0]
    if u.endswith("/"):
        u = u[:-1]
    return u


def save_as_md(title: str, url: str, content: str, filename_base: str) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—Å—Ç –≤ —Ñ–∞–π–ª¬†Markdown.

    filename_base ‚Äì —É–∂–µ –≥–æ—Ç–æ–≤–æ–µ ¬´—Ç–µ–ª–æ¬ª –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ *–±–µ–∑* —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
    (–Ω–∞–ø—Ä–∏–º–µ—Ä  `–≥–ª–∞–≤–∞-1-2-–µ—Å–ª–∏-‚Ä¶`).  
    –§—É–Ω–∫—Ü–∏—è —Å–∞–º–∞ –¥–æ–±–∞–≤–ª—è–µ—Ç —Å—É—Ñ—Ñ–∏–∫—Å `.md`.

    –ü—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º –≤—ã–∑–æ–≤–µ —Å —Ç–µ–º‚Äë–∂–µ –±–∞–∑–æ–≤—ã–º –∏–º–µ–Ω–µ–º –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ,
    —Ç–µ–º¬†—Å–∞–º—ã–º —É—Å—Ç—Ä–∞–Ω—è—è –¥—É–±–ª–∏.
    """
    filename = f"{filename_base}.md"
    subcat   = filename_base      # –ø–æ–ª–µ subcategory –≤ YAML‚Äë—Ñ—Ä–æ–Ω—Ç‚Äë–º–∞—Ç—Ç–µ—Ä–µ

    out_path = OUTPUT_DIR / filename
    if out_path.exists():         # —Ñ–∞–π–ª —É–∂–µ –µ—Å—Ç—å¬†‚Äì –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ–º
        return

    with open(out_path, "w", encoding="utf-8") as f:
        f.write(f"""---
category: {category}
section_1c: {section}
subcategory: {subcat}
question: {title}
url: {url}
---

{content.strip()}
""")
    print("‚úÖ", filename)


def extract_content_from_iframe(iframe):
    html = iframe.content()
    soup = BeautifulSoup(html, "html.parser")
    div = soup.select_one("div.doc-content, div#content") or soup.body
    return div.get_text("\n", strip=True) if div else ""


def split_into_sections(html: str) -> list[tuple[str, str]]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç HTML¬†–≥–ª–∞–≤—ã –Ω–∞ –ø–æ–¥‚Äë–±–ª–æ–∫–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º <h2>/<h3>.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (title, plain_text).
    """
    soup = BeautifulSoup(html, "html.parser")
    print("      ‚Ü≥ found headings:",
          [h.get_text(" ", strip=True) for h in soup.find_all(['h2', 'h3', 'strong', 'b'])][:15])
    sections: list[tuple[str, str]] = []

    current_title = None
    buffer: list[str] = []

    heading_tags = ("h1", "h2", "h3", "h4", "h5", "h6", "strong", "b")

    for node in soup.find_all(["h2", "h3", "strong", "b", "p"]):
        is_heading = (
            node.name in heading_tags
            or (
                node.name == "p"
                and (
                    "bold" in (node.get("class") or [])
                    or "font-weight:bold" in node.get("style", "").replace(" ", "").lower()
                    or (node.find("b") and len(node.get_text(strip=True)) <= 120)
                )
            )
        )

        if is_heading:
            if current_title and buffer:
                sections.append((current_title, "\n".join(buffer).strip()))
                buffer = []
            current_title = node.get_text(" ", strip=True)
            continue

        txt = node.get_text(" ", strip=True)
        if txt:
            buffer.append(txt)

    # —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –±—É—Ñ–µ—Ä
    if current_title and buffer:
        sections.append((current_title, "\n".join(buffer).strip()))
    return sections


def log_snippet(title: str, content: str):
    snippet = content.strip().replace("\n", " ")[:80]
    print(f"    ‚Ü≥ snippet: {snippet or '<EMPTY>'}")


def get_doc_iframe(page, timeout=15_000):
    """
    –ñ–¥—ë—Ç, –ø–æ–∫–∞ —Ä–∞–±–æ—á–∏–π iframe –ø–æ—è–≤–∏—Ç—Å—è –≤ DOM –∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–Ω–∏–º–µ—Ç —Å—Ç–∞—Ç—É—Å
    loading/hidden, –∑–∞—Ç–µ–º –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç Frame.
    """
    # iframe –ø–æ—è–≤–ª—è–µ—Ç—Å—è –≤ DOM —Å—Ä–∞–∑—É, –Ω–æ –ø–æ–∫–∞ —É –Ω–µ–≥–æ loading="true" ‚Äì¬†–æ–Ω hidden
    page.wait_for_selector(
        'iframe[name="w_metadata_doc_frame"]',
        state="attached",        # –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —á—Ç–æ–±—ã —ç–ª–µ–º–µ–Ω—Ç –±—ã–ª –≤ DOM
        timeout=timeout,
    )

    # –¥–æ–∂–∏–¥–∞–µ–º—Å—è –∫–æ–Ω—Ü–∞ –∑–∞–≥—Ä—É–∑–∫–∏: —ç–ª–µ–º–µ–Ω—Ç –≤–∏–¥–∏–º –∏ –Ω–µ—Ç loading="true"
    page.wait_for_function(
        """selector => {
               const el = document.querySelector(selector);
               return el && !el.hidden && el.getAttribute('loading') !== 'true';
           }""",
        arg='iframe[name="w_metadata_doc_frame"]',
        timeout=timeout
    )
    return page.frame(name="w_metadata_doc_frame")


def safe_goto(page, url: str):
    """
    –ü–µ—Ä–µ—Ö–æ–¥–∏—Ç –Ω–∞ url, –∂–¥—ë—Ç domcontentloaded –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç iframe —Å —Ç–µ–∫—Å—Ç–æ–º.
    –ï—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π iframe –ø—É—Å—Ç–æ–π, –ø—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ –Ω–µ–ø—É—Å—Ç–æ–π –¥–æ—á–µ—Ä–Ω–∏–π.
    """
    page.goto(url, timeout=30_000, wait_until="domcontentloaded")
    page.wait_for_timeout(300)        # –Ω–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –¥–ª—è —Å–∫—Ä–∏–ø—Ç–æ–≤
    try:
        iframe = get_doc_iframe(page)
    except Exception:
        print("‚ö†Ô∏è  iframe –Ω–µ –ø—Ä–æ–≥—Ä—É–∑–∏–ª—Å—è, –ø—Ä–æ–±—É—é –µ—â—ë —Ä–∞–∑ ‚Ä¶")
        page.wait_for_timeout(1000)
        iframe = get_doc_iframe(page)

    # ‚è≥¬†–ñ–¥—ë–º, –ø–æ–∫–∞ –≤–æ —Ñ—Ä–µ–π–º–µ –ø–æ—è–≤–∏—Ç—Å—è ¬´–∂–∏–≤–æ–π¬ª —Ç–µ–∫—Å—Ç ‚Äì —Ö–æ—Ç—è¬†–±—ã –æ–¥–∏–Ω
    # –∑–∞–≥–æ–ª–æ–≤–æ–∫¬†–∏–ª–∏ –∞–±–∑–∞—Ü.  –ë–µ–∑ —ç—Ç–æ–≥–æ .content() –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å
    # –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –ø—É—Å—Ç—É—é —Ä–∞–∑–º–µ—Ç–∫—É <script>‚Ä¶</script>.
    try:
        iframe.wait_for_selector("h1, h2, h3, p", timeout=10_000)
    except Exception:
        # –µ—Å–ª–∏ –ø–æ‚Äë–∫–∞–∫–æ–º—É‚Äë—Ç–æ –ø–æ–≤–æ–¥—É –Ω–µ –¥–æ–∂–¥–∞–ª–∏—Å—å ‚Äì –ø—Ä–æ–¥–æ–ª–∂–∏–º; –¥–∞–ª—å—à–µ
        # –≤—Å—ë‚Äë—Ä–∞–≤–Ω–æ –ø–æ–ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç.
        pass

    try:
        if iframe and not iframe.content().strip():
            for child in iframe.child_frames:
                if child.content().strip():
                    iframe = child
                    break
    except Exception:
        pass

    try:
        iframe.wait_for_selector("h1, h2, h3, p", timeout=5_000)
    except Exception:
        pass

    return iframe


def main():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False, slow_mo=100)
        context = browser.new_context()
        page = context.new_page()

        # 1. –ó–∞—â–∏—Ç–∞ + –ª–æ–≥–∏–Ω
        page.goto("https://its.1c.ru/")
        input("‚è≥ –ü—Ä–æ–π–¥–∏ –∑–∞—â–∏—Ç—É DDoS –∏ –Ω–∞–∂–º–∏ Enter...")
        print("üß≠ –¢–µ–∫—É—â–∏–π URL:", page.url)
        with open("debug_page_content.html", "w", encoding="utf-8") as f:
            f.write(page.content())

        # 2. –ü–µ—Ä–µ—Ö–æ–¥ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –£–ù–§
        page.goto(START_URL)
        page.wait_for_timeout(3000)  # –¥–∞—Ç—å –≤—Ä–µ–º—è –Ω–∞ –ø—Ä–æ–≥—Ä—É–∑–∫—É –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        print("üß≠ –ü–æ—Å–ª–µ –ª–æ–≥–∏–Ω–∞:", page.url)

        # DEBUG: —Å–æ—Ö—Ä–∞–Ω—è–µ–º HTML
        with open("debug_after_login.html", "w", encoding="utf-8") as f:
            f.write(page.content())

        print("üì¶ –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ—Ä–µ–π–º—ã:")
        for f in page.frames:
            print("  ‚Üí", f.name, f.url)
        try:
            page.wait_for_selector("iframe[name=w_metadata_doc_frame]", timeout=10000)
            iframe = page.frame(name="w_metadata_doc_frame")
            if not iframe:
                print("‚ùå iframe w_metadata_doc_frame –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return

            print(f"üß≠ iframe –∑–∞–≥—Ä—É–∂–µ–Ω: {iframe.url}")
            with open("debug_iframe_index.html", "w", encoding="utf-8") as f:
                f.write(iframe.content())
            soup = BeautifulSoup(iframe.content(), "html.parser")
            ul_count = len(soup.select("ul"))
            li_count = len(soup.select("li"))
            a_count = len(soup.select("a"))
            print(f"üìä –≠–ª–µ–º–µ–Ω—Ç—ã –≤ iframe: ul={ul_count}, li={li_count}, a={a_count}")

            print("üì• –ü—Ä–æ–≤–µ—Ä–∏–º HTML –¥–æ —Ä–∞—Å–∫—Ä—ã—Ç–∏—è –ø–∞–ø–æ–∫...")
            with open("debug_iframe_before_expand.html", "w", encoding="utf-8") as f:
                f.write(iframe.content())

            # ------------------------------------------------------------------
            #  TOC is rendered separately inside the main document, *not* inside
            #  the w_metadata_doc_frame iframe.  Once JS finishes, all <a> tags
            #  are already present in  #w_metadata_toc  ‚Äì even for collapsed
            #  branches ‚Äì so we just scrape them directly.
            # ------------------------------------------------------------------
            print("‚è≥  –ñ–¥—ë–º –∑–∞–≥—Ä—É–∑–∫—É –ø–æ–ª–Ω–æ–≥–æ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è ‚Ä¶")
            page.wait_for_selector(
                f'#w_metadata_toc a[href*="/db/{BOOK}/content/"]',
                timeout=15_000
            )

            print("üîç  –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ #w_metadata_toc ‚Ä¶")
            toc_links = page.evaluate(f'''
                Array.from(
                    document.querySelectorAll(
                        '#w_metadata_toc a[href*="/db/{BOOK}/content/"]'
                    )
                ).map(a => {{
                    const href = a.getAttribute('href') || '';
                    return {{
                        title : (a.textContent || '').trim(),
                        url   : href.startsWith('http') ? href
                               : new URL(href, 'https://its.1c.ru').href
                    }};
                }});
            ''')

            links = [
                link for link in toc_links
                if link["title"] and link["url"] and not link["url"].startswith("#")
            ]
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–∂–∏–¥–∞–Ω–∏—è —Å—Å—ã–ª–æ–∫ –≤ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–∏: {e}")
            with open("debug_unfdoc_page.html", "w", encoding="utf-8") as f:
                f.write(page.content())
            return

        print(f"üìã –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ –≤ –æ–≥–ª–∞–≤–ª–µ–Ω–∏–∏: {len(links)}")

        print(f"üß© –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(links)}")

        # ------------------------------------------------------------------
        #  –§–æ—Ä–º–∏—Ä—É–µ–º –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ ¬´—á–∏—Ç–∞–µ–º—ã–µ¬ª –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤.
        #  ‚Ä¢ –ì–ª–∞–≤–∞ 1  ‚Üí  –≥–ª–∞–≤–∞-1-<–Ω–∞–∑–≤–∞–Ω–∏–µ>
        #  ‚Ä¢ –ü–æ–¥‚Äë—Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤–Ω—É—Ç—Ä–∏ –≥–ª–∞–≤—ã 1  ‚Üí  –≥–ª–∞–≤–∞-1-1-<–Ω–∞–∑–≤–∞–Ω–∏–µ>, –≥–ª–∞–≤–∞-1-2-‚Ä¶
        # ------------------------------------------------------------------
        chapter_no: int | None = None
        sub_index:  int        = 0

        for ln in links:
            title = ln["title"].strip()
            m = re.match(r"–ì–ª–∞–≤–∞\s+(\d+)\.", title, flags=re.I)
            if m:
                chapter_no = int(m.group(1))
                sub_index  = 0
                rest = title[m.end():].strip()
                ln["fname_base"] = f"–≥–ª–∞–≤–∞-{chapter_no}-{sanitize_filename(rest)}"
            elif chapter_no is not None:
                sub_index += 1
                ln["fname_base"] = (
                    f"–≥–ª–∞–≤–∞-{chapter_no}-{sub_index}-" + sanitize_filename(title)
                )
            else:
                ln["fname_base"] = sanitize_filename(title)

        # --- –æ–±—Ö–æ–¥ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∫–Ω–∏–≥–∏ –≤ —à–∏—Ä–∏–Ω—É ---
        queue = deque(links)           # –Ω–∞—á–∏–Ω–∞–µ–º —Å –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è
        saved_urls = set()
        saved_titles: set[str] = set()

        while queue:
            link = queue.popleft()

            if not link.get("url"):
                continue

            norm_url = normalize_url(link["url"])
            file_base = link.get("fname_base") or sanitize_filename(link["title"])

            print(f"üîπ {link['title']} ‚Äî {link['url']}")
            iframe = safe_goto(page, link["url"])
            html = iframe.content()
            content = extract_content_from_iframe(iframe)
            log_snippet(link["title"], content)

            # --- —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∞–º—É –≥–ª–∞–≤—É ---
            if content:
                save_as_md(link["title"], link["url"], content, file_base)
                saved_titles.add(link["title"].strip())

            current_page_title = link["title"].strip()

            # --- —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø–æ–¥‚Äë—Ä–∞–∑–¥–µ–ª—ã –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã ---
            for idx, (sub_title, sub_text) in enumerate(split_into_sections(html), start=1):
                sub_url = f"{link['url']}#{sanitize_filename(sub_title)}"
                if sub_title == current_page_title or sub_url in saved_urls:
                    continue
                save_as_md(sub_title, sub_url, sub_text, f"{file_base}-{idx:02d}")
                saved_urls.add(sub_url)
                saved_titles.add(sub_title.strip())

            saved_urls.add(norm_url)

            # –∏—â–µ–º –≤—Å–µ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥—Ä—É–≥–∏–µ —Ä–∞–∑–¥–µ–ª—ã —Ç–æ–π –∂–µ –∫–Ω–∏–≥–∏
            try:
                raw_links = iframe.evaluate(
                    f'''
                    Array.from(document.querySelectorAll('a[href*="/db/{BOOK}/content/"]'))
                          .map(a => ({{
                              title: (a.textContent || '').trim(),
                              url: new URL(a.getAttribute('href'), "https://its.1c.ru").href
                          }}))
                    '''
                )
                queued = 0
                for nl in raw_links:
                    nurl = normalize_url(nl["url"], keep_fragment=True)
                    if nurl not in saved_urls:
                        nl["url"] = nurl
                        queue.append(nl)
                        queued += 1
                print(f"üîñ –í –æ—á–µ—Ä–µ–¥—å –¥–æ–±–∞–≤–ª–µ–Ω–æ: {queued}")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {e}")

        print("üèÅ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω. –ó–∞–∫—Ä—ã–≤–∞–µ–º –±—Ä–∞—É–∑–µ—Ä.")
        browser.close()unfdoc


if __name__ == "__main__":
    main()